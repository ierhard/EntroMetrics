% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropy_ci_Wald.R
\name{entropy_ci_Wald}
\alias{entropy_ci_Wald}
\title{Wald confidence interval (based on asymptotic normality)}
\usage{
entropy_ci_Wald(
  bin_counts,
  conf_level = 0.95,
  z_score_dist = c("t", "normal"),
  pt_est = NULL,
  prob_est = NULL,
  unit = c("log2", "ln", "normalize")
)
}
\arguments{
\item{bin_counts}{Vector of observed bin counts}

\item{conf_level}{Confidence level}

\item{z_score_dist}{Choose distribution from which to derive z-values: \code{"t"} (\eqn{t_{n-1}} distribution, where \eqn{n} is sample size; more conservative, especially recommended when \eqn{n \leq 30}) and \code{"normal"} (standard normal distribution)}

\item{pt_est}{Optional: Point estimate of entropy or a function (of bin counts) that returns a point estimate of entropy in bits. (For validity, the point estimator should share asymptotic variance with ML estimator for entropy, e.g. Millwer-Madow estimator.) By default ML estimator of entropy is used.}

\item{prob_est}{Optional: Vector of bin probabilites or a function (of bin counts) that returns a vector of bin probabilities. By default ML estimator of probabilities is used.}
}
\description{
Confidence interval based on the Wald method (normality assumption) for the entropy of a distribution and standard error estimates of the form \eqn{\hat S_n = \sqrt{\frac 1n \left[\big( \sum_{i=1}^s \hat p_i(\log_2 \hat p_i)^2\big) - \hat H^2 \right]}} due to Basharin's proof of asymptotic normality and \eqn{O(1/n^2)} approximation of the variance of the ML entropy estimator.
}
