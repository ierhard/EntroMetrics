# Data generation

Load necessary libraries

```{r}
library(gtools)
library(purrr)
library(dplyr)
library(data.table)
```

Set seed
```{r}
set.seed(20241212)
```


## Distribution and sample generation

We begin by randomly generating sample sizes and distributions according to the following schema $10^6$ times:

- Sample size: $N \sim \lfloor 10^{U(0,3) )}\rfloor + 4$ \\ 
*(ensures sample sizes of at least $5$)*
- Alphabet size: $K \sim \lfloor 10^{U(0,4) )}\rfloor + 1$ \\ 
*(ensures alphabets sizes of at least $2$)*
- Distribution over alphabet: $\mathbf p \sim \operatorname{Dirichlet}\big(\boldsymbol \alpha = (\underbrace{1, \dots, 1}_{K}) \big)$
- Sample: $\mathbf X \sim \operatorname{Multinomial}(N; \mathbf p)$

Note that the random variables above should be independent or conditionally independent in the obvious ways.

### Batch generation function

Define a function to generate an arbitrary number of samples.
```{r}
# Data generation function with data.table output
generate_batch_dt <- function(batch_size = 10^4,
                              max_sample_size = 10^3,
                              max_alphabet_size = 10^4) {
  
  # Vector of sample sizes
  max_sample_order_magnitude <- log10(max_sample_size)
  
  N_vec <- floor(
    10^runif(batch_size,
             min = 0, 
             max = max_sample_order_magnitude)
    ) + 4
  
  # Vector of alphabet sizes
  max_alphabet_order_magnitude <- log10(max_alphabet_size)
  
  K_vec <- floor(
    10^runif(batch_size, 
             min = 0, 
             max = max_alphabet_order_magnitude)
    ) + 1
  
  # List of distributions
  p_list <- purrr::map(K_vec, 
                       function(K) gtools::rdirichlet(1, rep(1, K))[1,])
  
  # List of samples
  X_list <- purrr::map2(N_vec, p_list, 
                        function(N, p) rmultinom(1, N, p)[,1] )
  
  # Combine into a data.table
  dt <- data.table::data.table(
    N = N_vec,
    K = K_vec,
    p = p_list,
    X = X_list
  )
  
  return(dt)
}
```

### Generation

In this section we:
1. Set parameters for the scope of data generation
2. Generate the data in batches (for memory efficiency)
3. Combine the batches, and save the data to disk.

Begin by setting the parameters.

```{r}
# Training data sample size
train_size <- 10^4

# Test data sample size
test_size <- 10^3

# Batch size
batch_size <- (10^4)/2

# Largest sample and alphabet sizes (for individual bin counts)
max_sample_size <- 10^3
max_alphabet_size <-  10^3
```

Generate the training data in batches and save to disk.

```{r}
# Number of batches
num_batches <- ceiling(train_size / batch_size)

# Output directory for batches
output_dir <- "data_batches/"
dir.create(output_dir, showWarnings = FALSE)

# Progress bar
pb <- txtProgressBar(min = 0, max = num_batches, style = 3)

# Generate and save batches
for (i in 1:num_batches) {
  # Update progress bar
  setTxtProgressBar(pb, i)
  
  # Generate batch
  batch <- generate_batch_dt(batch_size,
                             max_sample_size = 10^3,
                             max_alphabet_size = 10^4)
  
  # Save the batch as a separate RDS file
  saveRDS(batch, 
          file = file.path(output_dir, paste0("batch_", i, ".rds")))

  rm(batch)
  
}

# Close progress bar
close(pb)

# Garbage collection for memory efficiency
gc()
```

Combined the batches and save the data to disk.

```{r}
# Combine all batches into a single file
combined_data <- do.call(
  rbind, 
  lapply(list.files(output_dir,
                    pattern = "batch_.*\\.rds", 
                    full.names = TRUE), 
         readRDS)
)

# Save the combined data
saveRDS(combined_data, "data_train_combined.rds")

# Garbage collection
gc()
```

## Calculate features and response variables

The basic data is now generated and saved; however, features, response variables, and ancillary information must still be calculated.

**Features**
- Sample size: $N$
- Alphabet size: $K$
- Sample to alphabet ratio (STA): $N/K$
- Total variation of empirical distribution to uniform distribution: $d_{\text{TV}}\big( \hat{\mathbf{p}}, (1/K, \dots, 1/K) \big) = \frac{1}{2} \sum_{k=1}^K |\hat p_k - 1/K|$
- *... (more to come, perhaps $R^2$ metrics for fit of data to power law distribution or exponential decay distribution)*
- True entropy: $H(\mathbf p) = -\sum_{k=1}^K p_k \log p_k$
- Empirical various entropy point estimates $\hat H_j(\mathbf X)$ for $j = 1, \dots, m$
- Best entropy point estimate $\hat k \equiv \hat k(\mathbf X) := \operatorname{argmin}_{j = 1, \dots, m} |\hat H_j(\mathbf X) - H(\mathbf p)|$

```{r}
# Sample to alphabet ratio (STA)
combined_data[, STA := N/K]

# Total variation of empirical distribution to uniform distribution
combined_data[, d_TV := map2_dbl(p, K,
                                 function(p_elt, K_elt){
                                   0.5 * sum(abs(p_elt - K_elt))
                                 })]

# True entropy
combined_data[, H_true := map_dbl(p, 
                                  function(p_elt){
                                    p_elt <- p_elt[p_elt > 0]
                                    -sum(p_elt * log2(p_elt))
                                  })]

```

Now loop over all the point estimates that we wish to consider.

```{r}
# CODE HERE
```






















































